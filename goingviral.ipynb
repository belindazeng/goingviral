{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Popularity: Using Text and Content Analysis to Examine Shared Characteristics of Popular Posts on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A CS109 Final Project by Belinda Zeng, Roseanne Feng, Yuqi Hou, and Zahra Mahmood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://studentshare.net/content/wp-content/uploads/2015/05/53a0e7d640b31_-_unknown-3-51047042.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter (https://twitter.com) is social network, real-time news media service, and micro-blogging service where users can use text, photos, and videos to express moments or ideas in 140-characters or less. These 140-character messages are called \"tweets.” According to Twitter’s website, millions of tweets are shared in real time, every day. Registered users can read and post tweets, favorite other people’s tweets, retweet other people’s posts, favorite tweets, and follow other accounts. Unregistered users can read tweets from public accounts. \n",
    "\n",
    "In today's day and age of Twitter, popularity is measured in hearts, retweets, follows, and follow-backs. What posts get popular over time? What seems to resonate most with people? Do positive or negative sentiments invite more engagement? In this project, we use Twitter's publically available archive of content to  like to examine some of the shared characteristics of popular posts, including length of post, visual content, positivity, negativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea came from a desire to understand how movements such as #BlackLivesMatter and #Ferguson begin on Twitter as well as a general desire to know what makes a post popular. We chose to focus on Tweets on an individual level and to use natural language processing to be able to understand and predict what makes posts popular.\n",
    "\n",
    "One paper that is related to our work is a paper from Cornell titled, [The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter](https://chenhaot.com/pages/wording-for-propagation.html), which compaired pairs of tweets containing the same url and written by the same user but employing different wording to see which version attracted more retweets. Twitter itself has published research on [What fuels a Tweet’s engagement?](https://blog.twitter.com/2014/what-fuels-a-tweets-engagement) Their research found that adding video, links and photos all result in an increase in the number of retweets and even breaking down those results by industry. Inspired by previous research, we sought to include sentiment analysis in our understanding of what made a Tweet popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does the distribution of retweets and hearts vary for a post depending on the time of day when tweet is created?\n",
    "2. How does positive and negative sentiment affect popularity? \n",
    "3. What Tweets do we think will become popular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is publicly available via the Twitter Static API that gets queries based on specific parameters. We limited the data set to look at tweets within a specified period of time. We are storing the data in CSV files for now. To reduce file-sizes, we will try to have multiple CSVs so that we don't load too much data into memory. If data exceeds computer memory, we will consider AWS/SQL database alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up oauth and a app on Twitter (to getthe consumer key & secret and access token and secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# great resource where I got all this \n",
    "# http://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/\n",
    "\n",
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "\n",
    "consumer_key = 'lun6TR6KpaISisFdGnQ5Eo8v5'\n",
    "consumer_secret = 'hmwEtnfvTfI6CljEKKtIGjahG4NcFQvLBXhOnPyFHmAqNZ9fVV'\n",
    "access_token = '3004335028-UKSgKFDbaBLNWTzXQFrBRDwVOKo0JR475KYY3LW'\n",
    "access_secret = 'pA6MeW4NYsv3tL0MRvjI1oBqdUZc0os11gesdNVkeLpX2'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial approach is to create a random sample that consists of 1% of tweets. This involves using tweepy and the sample call from the Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "# final, final version \n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "# get retweet status\n",
    "def try_retweet(status, attribute):\n",
    "    try:\n",
    "        if getattr(status, attribute):\n",
    "            return True\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# function that tries to get attribute from object\n",
    "def try_get(status, attribute):\n",
    "    try:\n",
    "        return getattr(status, attribute).encode('utf-8')\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# open csv file\n",
    "csvFile = open('smallsample.csv', 'a')\n",
    "\n",
    "# create csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        try:\n",
    "            # save relevant components of the tweet\n",
    "            \n",
    "            # get and sanitize hashtags \n",
    "            hashtags = status.entities['hashtags']\n",
    "            hashtag_list = []\n",
    "            for el in hashtags:\n",
    "                hashtag_list.append(el['text'])\n",
    "            hashtag_count = len(hashtag_list)\n",
    "            \n",
    "            # get and sanitize urls\n",
    "            urls = status.entities['urls']\n",
    "            url_list = []\n",
    "            for el in urls:\n",
    "                url_list.append(el['url'])\n",
    "            url_count = len(url_list)\n",
    "            \n",
    "            # get and sanitize user_mentions\n",
    "            user_mentions = status.entities['user_mentions']\n",
    "            mention_list = []\n",
    "            for el in user_mentions:\n",
    "                mention_list.append(el['screen_name'])\n",
    "            mention_count = len(mention_list)\n",
    "            # save it all as a tweet\n",
    "            tweet = [status.created_at, status.text.encode('utf-8'), status.place, status.lang, status.coordinates, \n",
    "              hashtag_list, url_list, mention_list, \n",
    "              hashtag_count, url_count, mention_count, \n",
    "              try_get(status, 'possibly_sensitive'),\n",
    "              status.favorite_count, status.favorited, status.retweet_count, status.retweeted, \n",
    "              try_retweet(status,'retweeted_status'), \n",
    "              try_get(status.user, 'statuses_count'), \n",
    "              try_get(status.user, 'favourites_count'), \n",
    "              try_get(status.user, 'followers_count'),\n",
    "              try_get(status.user, 'description'),\n",
    "              try_get(status.user, 'location')]\n",
    "            \n",
    "            # write to csv\n",
    "            csvWriter.writerow(tweet)\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    "    \n",
    "    # tell us if there's an error\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.sample()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, analysis will be done previously scraped tweets, and there is no need to run the above code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetdf_small=pd.read_csv(\"tempdata/smallsample.csv\", names=[\"created_at\", \"text\", \"place\", \"lang\", \"coordinates\",\n",
    "                                       \"hashtags\", \"urls\", \"user_mentions\", \n",
    "                                       \"hashtag_count\", \"url_count\", \"mention_count\",\n",
    "                                       \"possibly_sensitive\", \n",
    "                                       \"favorite_count\", \"favorited\", \"retweet_count\", \"retweeted\",\n",
    "                                       \"retweeted_status\", \"user_statuses_count\", \"user_favorites_count\",\n",
    "                                       \"user_follower_count\", \"user_description\", \"user_location\"])\n",
    "tweetdf_small.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetdf_small.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see however, the retweet count and favorite count are always 0. This is because we're using the live streaming API and as a result, we're scraping the tweets as they are tweeted. At this point, all the tweets have retweet count 0 and favorite count 0 since they were literally just posted! That is, unless the tweet posted is actually a retweet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just found this bug with retweet_count, looking into why this might be the case\n",
    "tweetdf_missing = tweetdf_small[tweetdf_small['retweet_count'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetdf_missing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting original retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function updates the way we use the tweepy streaming API. We first detect if the tweet we're looking at is actually a retweet of something. If so, we then get the original tweet and save that to our csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "# only save information for retweets\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "# get retweet status\n",
    "def try_retweet(status, attribute):\n",
    "    try:\n",
    "        if getattr(status, attribute):\n",
    "            return True\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# get country status\n",
    "def try_country(status, attribute):\n",
    "    if getattr(status, attribute) != None:\n",
    "        place = getattr(status, attribute)\n",
    "        return place.country\n",
    "    return None\n",
    "\n",
    "# get city status\n",
    "def try_city(status, attribute):\n",
    "    if getattr(status, attribute) != None:\n",
    "        place = getattr(status, attribute)\n",
    "        return place.full_name\n",
    "    return None\n",
    "\n",
    "# function that tries to get attribute from object\n",
    "def try_get(status, attribute):\n",
    "    try:\n",
    "        return getattr(status, attribute).encode('utf-8')\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# open csv file\n",
    "csvFile = open('originalsample.csv', 'a')\n",
    "\n",
    "# create csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        try:\n",
    "            # if this represents a retweet\n",
    "            if try_retweet(status,'retweeted_status'):\n",
    "                status = status.retweeted_status\n",
    "                \n",
    "                # get and sanitize hashtags \n",
    "                hashtags = status.entities['hashtags']\n",
    "                hashtag_list = []\n",
    "                for el in hashtags:\n",
    "                    hashtag_list.append(el['text'])\n",
    "                hashtag_count = len(hashtag_list)\n",
    "\n",
    "                # get and sanitize urls\n",
    "                urls = status.entities['urls']\n",
    "                url_list = []\n",
    "                for el in urls:\n",
    "                    url_list.append(el['url'])\n",
    "                url_count = len(url_list)\n",
    "\n",
    "                # get and sanitize user_mentions\n",
    "                user_mentions = status.entities['user_mentions']\n",
    "                mention_list = []\n",
    "                for el in user_mentions:\n",
    "                    mention_list.append(el['screen_name'])\n",
    "                mention_count = len(mention_list)\n",
    "                \n",
    "                # save it all as a tweet\n",
    "                tweet = [status.id, status.created_at, try_country(status, 'place'), try_city(status, 'place'), status.text.encode('utf-8'), status.lang,\n",
    "                  hashtag_list, url_list, mention_list, \n",
    "                  hashtag_count, url_count, mention_count, \n",
    "                  try_get(status, 'possibly_sensitive'),\n",
    "                  status.favorite_count, status.favorited, status.retweet_count, status.retweeted, \n",
    "                  status.user.statuses_count, \n",
    "                  status.user.favourites_count, \n",
    "                  status.user.followers_count,\n",
    "                  try_get(status.user, 'description'),\n",
    "                  try_get(status.user, 'location'),\n",
    "                  try_get(status.user, 'time_zone')]\n",
    "            \n",
    "                # write to csv\n",
    "                csvWriter.writerow(tweet)\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    "    \n",
    "    # tell us if there's an error\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.sample()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read into pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetdf=pd.read_csv(\"tempdata/originalsample.csv\", names=[\"id\", \"created_at\", \"country\", \"city\", \"text\", \"lang\",\n",
    "                                       \"hashtags\", \"urls\", \"user_mentions\", \n",
    "                                       \"hashtag_count\", \"url_count\", \"mention_count\",\n",
    "                                       \"possibly_sensitive\", \n",
    "                                       \"favorite_count\", \"favorited\", \"retweet_count\", \"retweeted\",\n",
    "                                       \"user_statuses_count\", \"user_favorites_count\",\n",
    "                                       \"user_follower_count\", \"user_description\", \"user_location\", \"user_timezone\"])\n",
    "tweetdf.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_filtered = tweetdf[tweetdf['lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for unique tweet ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_filtered.drop_duplicates(subset='id', take_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popularity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popularity = [retweets + favs for retweets, favs in zip(df_filtered.retweet_count, df_filtered.favorite_count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add popularity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_filtered['popularity']=popularity\n",
    "df_filtered.loc[:,'popularity']=popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftouse = df_filtered.reset_index()\n",
    "dftouse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scraping the tweets from the Twitter API, we can use that data to build a feature list that we use to predict how popular an individual tweet is, measured by a composite score based on the amount of retweets and hearts. We will also use metadata to help us analyze trends in the data, for example if there is a correlation between time of day and retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update: 11/30 - 12/1 (Yuqi)\n",
    "\n",
    "Initial exploratory analysis regarding popularity score and hashtags done. It seems like we should rethink our current formula for popularity because the histogram gives extreme strange results and the max score is really high. Need to look into why that might be. \n",
    "\n",
    "All of the correlations that were done between popularity score and other factors came up significant. Could this be due to the large dataset that we are using? Should we be worried about things being labeled as significant not because it actually is significant but because there is so much data that small variations become significant?\n",
    "\n",
    "Also, noticed that some tweets are longer than 140 characters, and I'm not sure why that is either. Further data wrangling probably needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity Score Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rethink how popularity is scored? ##\n",
    "Huge standard deviation and extreme ranges suggest that we may need to rethink how we score popularity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftouse['popularity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(dftouse['popularity'],bins=100)\n",
    "plt.title(\"Distribution of Popularity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(dftouse['retweet_count'],bins=100)\n",
    "plt.title(\"Distribution of Retweet Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(dftouse['favorite_count'],bins=100)\n",
    "plt.title(\"Distribution of Favorite Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftouse['retweet_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retweet_stats = dftouse['retweet_count'].describe()\n",
    "retweet_mean = retweet_stats[1]\n",
    "retweet_std = retweet_stats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dftouse['favorite_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "favorite_stats = dftouse['favorite_count'].describe()\n",
    "favorite_mean = favorite_stats[1]\n",
    "favorite_std = favorite_stats[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these statistics on retweet_count and favorite_count, we realize we want to standardize these two for use later on, otherwise since there are way more retweets than favorites, retweets would get weighted more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftouse = dftouse.rename(columns={'retweet_count': 'retweet_unstandardized', 'favorite_count': 'favorite_unstandardized'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create standardized retweet_count and favorite_count **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize retweet count and favorites by subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retweets = [(retweet_count - retweet_mean)/float(retweet_std) for retweet_count in dftouse['retweet_unstandardized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "favorites = [(favorite_count - favorite_mean)/float(favorite_std) for favorite_count in dftouse['favorite_unstandardized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add these as columns to our dftouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftouse.loc[:,'retweet_count']=retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftouse.loc[:,'favorite_count']=favorites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we recalculate popularity, but in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popularity = [retweets + favs for retweets, favs in zip(df_filtered.retweet_count, df_filtered.favorite_count)]\n",
    "dftouse.loc[:,'popularity']=popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftouse['popularity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "- http://stackoverflow.com/questions/1894269/convert-string-representation-of-list-to-list-in-python\n",
    "- http://stackoverflow.com/questions/10201977/how-to-reverse-tuples-in-python\n",
    "- http://stackoverflow.com/questions/13925251/python-bar-plot-from-list-of-tuples/34013980#34013980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What fraction of tweets in the sample use hashtags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_tags_per_tweet = dftouse['hashtag_count']\n",
    "tags_per_tweet = np.array(num_tags_per_tweet)\n",
    "tagfrac = float(len(tags_per_tweet[tags_per_tweet>0]))/float(len(tags_per_tweet))\n",
    "print str(tagfrac)+\" of tweets in the sample use one or more hashtags.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(tags_per_tweet)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Hashtags Used in Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 hashtags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get a flattened list of all the hashtags used in the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alltags=[] \n",
    "for i in dftouse['hashtags']: # grab all the tags and put them into a list\n",
    "    tag = ast.literal_eval(i) # convert string representation of list to list \n",
    "    alltags.append(tag) \n",
    "hashtags = [item for sublist in alltags for item in sublist] # flatten out the nested list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make a bar plot of the 10 most commonly used hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashfreq = Counter(hashtags) # get the frequency of appearing hashtags\n",
    "commontags = hashfreq.most_common(10) # save the top ten most common hashtags\n",
    "taglabels = zip(*commontags)[0][::-1] # reverse the tuples to go from most frequent to least frequent \n",
    "hashtaglabels = ['#'+i for i in taglabels] # add a pound sign in front of each tag to make it clear that it's a hashtag\n",
    "y_pos = np.arange(len(hashtaglabels)) \n",
    "usefreq = zip(*commontags)[1][::-1] # get the frequency part of the tuple\n",
    "plt.barh(y_pos, usefreq, align='center') # plot horizontal barplot\n",
    "plt.yticks(y_pos, hashtaglabels) \n",
    "plt.title('Top 20 Occuring Hashtags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Hashtags Associated with Highest Popularity Score Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pearsonr(dftouse['hashtag_count'],dftouse['popularity'])\n",
    "plt.scatter(dftouse['hashtag_count'],dftouse['popularity'])\n",
    "plt.ylabel('Popularity Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between length of tweet and popularity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More data wrangling possibly needed: Why are some tweets longer than 140 characters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_len = [len(text) for text in dftouse['text']]\n",
    "print pearsonr(tweet_len,dftouse['popularity'])\n",
    "plt.scatter(tweet_len,dftouse['popularity'])\n",
    "plt.ylabel('Popularity Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Update 12/4 (Yuqi): Tweets that have emojiis are converted into characters that's throwing off tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_len_array = np.array(tweet_len)\n",
    "idx = np.where(tweet_len_array > 140)[0].tolist()\n",
    "df_filtered_by_length = dftouse['text'].filter(idx).copy()\n",
    "df_over140 = df_filtered_by_length.reset_index()\n",
    "df_over140['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between presence of image and popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dataframe only has information about links, so not differentiating between images and other urls for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between presence of links and popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pearsonr(dftouse['url_count'],dftouse['popularity'])\n",
    "plt.scatter(dftouse['url_count'],dftouse['popularity'])\n",
    "plt.ylabel('Popularity Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between user mentions and popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pearsonr(dftouse['mention_count'],dftouse['popularity'])\n",
    "plt.scatter(dftouse['mention_count'],dftouse['popularity'])\n",
    "plt.ylabel('Popularity Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation for number of retweets and hearts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pearsonr(dftouse['retweet_count'],dftouse['favorite_count'])\n",
    "plt.scatter(dftouse['retweet_count'],dftouse['favorite_count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update: 12/4 (Yuqi)\n",
    "Originally we had planned to do exploratory analysis on popular topics that people tweet about by city or state, but after taking a look at our data, we found that 3.2% of tweets were geo-tagged, so we ultimately chose to forego this analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fraction of Tweets that are Geo-tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totaltweets = float(len(dftouse['country'])) # total number of tweets in sample\n",
    "countryfrac = float(sum(map(lambda r: int(isinstance(r, str)), dftouse['country'])))/totaltweets\n",
    "cityfrac = float(sum(map(lambda r: int(isinstance(r, str)), dftouse['city'])))/totaltweets\n",
    "print str(cityfrac)+\" of tweets in the sample are geo-tagged with a city.\"\n",
    "print str(countryfrac)+\" of tweets in the sample are geo-tagged with a country.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "date_objects = [datetime.strptime(each, '%Y-%m-%d %H:%M:%S') for each in dftouse['created_at']]\n",
    "dir(date_objects[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When are tweets posted throughout the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_objects = [each.weekday() for each in date_objects]\n",
    "plt.hist(day_objects)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When are tweets posted during the day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour_objects = [each.hour for each in date_objects]\n",
    "# plt.hist(hour_objects)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Counter(hour_objects)\n",
    "sum(hour_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 24 # number of bars should be 24 since there are 24 hours in a day\n",
    "bottom = 4 # determines how big the circle in the middle is \n",
    "max_height = 8\n",
    "\n",
    "theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)\n",
    "radii = max_height*np.random.rand(N)\n",
    "width = (2*np.pi) / N\n",
    "ax = plt.subplot(111, polar=True)\n",
    "ax.set_theta_direction(-1)\n",
    "bars = ax.bar(theta, radii, width=width, bottom=bottom)\n",
    "\n",
    "# Use custom colors and opacity\n",
    "for r, bar in zip(radii, bars):\n",
    "    bar.set_facecolor(plt.cm.jet(r / 10.))\n",
    "    bar.set_alpha(0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as tkr\n",
    "\n",
    "def realign_polar_xticks(ax):\n",
    "    for theta, label in zip(ax.get_xticks(), ax.get_xticklabels()):\n",
    "        theta = theta * ax.get_theta_direction() + ax.get_theta_offset()\n",
    "        theta = np.pi/2 - theta\n",
    "        y, x = np.cos(theta), np.sin(theta)\n",
    "        if x >= 0.1:\n",
    "            label.set_horizontalalignment('left')\n",
    "        if x <= -0.1:\n",
    "            label.set_horizontalalignment('right')\n",
    "        if y >= 0.5:\n",
    "            label.set_verticalalignment('bottom')\n",
    "        if y <= -0.5:\n",
    "            label.set_verticalalignment('top')\n",
    "\n",
    "def plot_clock(data):\n",
    "    def hour_formatAM(x, p):\n",
    "        hour = x * 6 / np.pi\n",
    "        return '{:0.0f}:00'.format(hour) if x > 0 else '12:00'\n",
    "\n",
    "    def hour_formatPM(x, p):\n",
    "        hour = x * 6 / np.pi\n",
    "        return '{:0.0f}:00'.format(hour + 12) if x > 0 else '24:00'\n",
    "\n",
    "    def plot(ax, theta, counts, formatter):\n",
    "        colors = plt.cm.jet(theta / 12.0)\n",
    "        ax.bar(theta, counts, width=np.pi/6, color=colors, alpha=0.5)\n",
    "        ax.xaxis.set_major_formatter(tkr.FuncFormatter(formatter))\n",
    "\n",
    "    bins = np.r_[0, 0.5:12, 12, 12.5:24,  23.99999]\n",
    "    data = np.array(data) / (60*60)\n",
    "    counts = np.histogram(data,bins)[0]\n",
    "\n",
    "    counts[13] += counts[0]\n",
    "    counts[-1] += counts[13]\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(20, 10), subplot_kw=dict(projection='polar'))\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set(theta_offset=np.pi/2, theta_direction=-1,\n",
    "               xticks=np.arange(0, np.pi*2, np.pi/6),\n",
    "               yticks=np.arange(1, counts.max()))\n",
    "\n",
    "    plot(axes[0], bins[1:13] * np.pi / 6, counts[1:13], hour_formatAM)\n",
    "#     plot(axes[1], bins[14:26] * np.pi / 6, counts[14:26], hour_formatPM)\n",
    "    return axes\n",
    "\n",
    "data = [ 10.49531611,  22.49511583,  10.90891806,  18.99525417,\n",
    "        21.57165972,   6.687755  ,   6.52137028,  15.86534639,\n",
    "        18.53823556,   6.32563583,  12.99365833,  11.06817056,\n",
    "        17.29261306,  15.31288556,  19.16236667,  10.38483333,\n",
    "        14.51442222,  17.01413611,   6.96102278,  15.98508611,\n",
    "        16.5287    ,  15.26533889,  20.83520278,  17.21952056,\n",
    "         7.3225775 ,  16.42534361,  14.38649722,  21.63573111,  16.19249444]\n",
    "data = np.array(data)*60*60\n",
    "print len(data), data\n",
    "axes = plot_clock(data)\n",
    "for ax in axes:\n",
    "    realign_polar_xticks(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between time of day and tweet popularity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot_date(date_objects, popularity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The distribution of retweets over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot_date(date_objects, dftouse['retweet_count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The distribution of hearts & retweets over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot_date(date_objects, dftouse['favorite_count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User's followers correlated with popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_follower_count = dftouse['user_follower_count'] \n",
    "print pearsonr(user_follower_count,popularity)\n",
    "plt.scatter(user_follower_count,popularity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending tweets and trending lists affecting virality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining positive/negative words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sentiment lookup dictionaries, score tweets based on how positive/negative they are.\n",
    "\n",
    "**11/29 - Roseanne**\n",
    "Used a basic list of positive/negative words to begin with, no weights or other information beyond positive/negative. Appears to miss a bunch of tokens (1812/892606 found).\n",
    "\n",
    "**12/1 - Roseanne**\n",
    "Tried LabMT, using code provided. Rate is a lot better (7016/892606).\n",
    "\n",
    "**12/4 - Roseanne**\n",
    "Realized number of tokens (892606) was total tokens instead of unique tokens (83093). Still a lot but more tokens found than expected. LabMT is probably the better choice, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#notes: Unicode in texts (probably emoticons? should we find a way to categorize those?)\n",
    "#df_filtered['text']\n",
    "\n",
    "#load dicts into lookup, map words to pos or neg value\n",
    "#current dict: not sure where it's from?\n",
    "#1812 of 83093 words in lookup.\n",
    "lookup = {}\n",
    "with open('positive.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word = line[:-1]\n",
    "        lookup[word] = 1\n",
    "with open('negative.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word = line[:-1]\n",
    "        lookup[word] = -1\n",
    "\n",
    "# uses LabMT for scoring, see http://neuro.imm.dtu.dk/wiki/LabMT\n",
    "# 7016 of 83093 words in LabMT.\n",
    "url = 'http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0026752.s001'\n",
    "labmt = pd.read_csv(url, skiprows=2, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# you'll need to download NLTK resource: nltk.download()\n",
    "# or use terminal: sudo python -m nltk.downloader -d /usr/local/share/nltk_data all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text = reduce(lambda x,y: x+y, dftouse['text'].apply(lambda x: [x])) # list of strings, functionally identical to dftouse['text']\n",
    "tweetstext = reduce(lambda x,y: x + '\\n' + y, dftouse['text']) # string of concatenated texts, all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out stop words, etc\n",
    "# notice: tokenizer puts punctuation as their own tokens, ex. separates hashtags, etc.\n",
    "tokens = nltk.word_tokenize(tweetstext.decode('utf-8','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Number of tokens:\", len(tokens)\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "utokens = fdist.keys()\n",
    "print \"Unique tokens:\", len(utokens)\n",
    "print \"Tokens that appear only once:\", len(fdist.hapaxes())\n",
    "#fdist.most_common(50)\n",
    "inlookup = []\n",
    "notfoundlookup = []\n",
    "inlabmt = []\n",
    "notfoundlabmt = []\n",
    "for key in utokens:\n",
    "    if key in lookup.keys():\n",
    "        inlookup.append(key)\n",
    "    else:\n",
    "        notfoundlookup.append(key)\n",
    "    if key in labmt.index:\n",
    "        inlabmt.append(key)\n",
    "    else:\n",
    "        notfoundlabmt.append(key)\n",
    "print \"{} of {} words in lookup.\".format(len(inlookup), len(utokens))\n",
    "print inlookup[:10]\n",
    "\n",
    "print \"{} of {} words in LabMT.\".format(len(inlabmt), len(utokens))\n",
    "print inlabmt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigrams = dftouse['text'].apply(lambda x: list(nltk.bigrams(nltk.word_tokenize(x.decode('utf-8','ignore')))))\n",
    "trigrams = dftouse['text'].apply(lambda x: list(nltk.trigrams(nltk.word_tokenize(x.decode('utf-8','ignore')))))\n",
    "trigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12/4 - Roseanne**\n",
    "\n",
    "Scoring - build columns for scoring text, one on the raw text, one on text that ignores words not in our dictionary, and one that shows us which words are not in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# average of entire tweet over unigrams\n",
    "average = labmt.happiness_average.mean()\n",
    "happiness = (labmt.happiness_average - average).to_dict()\n",
    " \n",
    "def score(text):\n",
    "    words = nltk.word_tokenize(text.decode('utf-8','ignore'))\n",
    "    return sum([happiness.get(word.lower(), 0.0) for word in words]) / len(words)\n",
    "\n",
    "def scoreNoNeutrals(text):\n",
    "    words = nltk.word_tokenize(text.decode('utf-8','ignore'))\n",
    "    notscored = [word for word in words if happiness.get(word.lower(), 0.0) == 0.0]\n",
    "    return sum([happiness.get(word.lower(), 0.0) for word in words]) / max((len(words) - len(notscored)),1)\n",
    "\n",
    "def notScored(text):\n",
    "    words = nltk.word_tokenize(text.decode('utf-8','ignore'))\n",
    "    return [word for word in words if happiness.get(word.lower(), 0.0) == 0.0]\n",
    "\n",
    "\n",
    "#dftouse['text'].apply(score).mean()\n",
    "dftouse['sentiment'] = dftouse['text'].apply(score)\n",
    "dftouse['sentimentnoneutrals'] = dftouse['text'].apply(scoreNoNeutrals)\n",
    "dftouse['notscored'] = dftouse['text'].apply(notScored)\n",
    "dftouse[['text','sentiment', 'sentimentnoneutrals', 'notscored']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12/4 - Roseanne**\n",
    "\n",
    "Checking how our lookup and scoring is working.\n",
    "\n",
    "Sentiment score ranges from approx. -3 to 3, with a mean close to 0.1, or roughly neutral.\n",
    "\n",
    "Hapaxes (words that appear only once in the Tweets we're analyzing) are a surprisingly large percentage of our tokens (~55000 out of 83000). A lot of them are URLs (19812), which we can probably ignore, or include a Unicode character or formatting that caused the tokenizer to behave oddly. Would it be worth it to try to filter out punctuation, or manually add them to our lookup (ex. replace .!?s with spaces, or add tokens such as '...'. If we add them, how do we generate a score for them?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dftouse.sentiment.min(), dftouse.sentiment.max(), dftouse.sentiment.mean()\n",
    "fdist.hapaxes() #lots of links, Unicode included here, is it worth filtering out these/punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utokens_ = [x for x in utokens if x[:6] != '//t.co']\n",
    "urltokens = [x for x in utokens if x[:6] == '//t.co']\n",
    "print \"Non-URL tokens:\", len(utokens_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dftouse.sentiment.min(), dftouse.sentiment.max(), dftouse.sentiment.mean()\n",
    "print dftouse.sentimentnoneutrals.min(), dftouse.sentimentnoneutrals.max(), dftouse.sentimentnoneutrals.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftouse.loc[dftouse.sentimentnoneutrals==dftouse.sentimentnoneutrals.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftouse.loc[dftouse.sentimentnoneutrals==dftouse.sentimentnoneutrals.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word, freq in fdist.most_common(50):\n",
    "    print word, score(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scoring the association\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print scored[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finds most associated bigrams\n",
    "top_bigrams = finder.nbest(bigram_measures.raw_freq, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define bigrams\n",
    "bigrams = dftouse['text'].apply(lambda x: list(nltk.bigrams(nltk.word_tokenize(x.decode('utf-8','ignore')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finds the most common 20\n",
    "import string\n",
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS\n",
    "punctuation = string.punctuation[:6] + string.punctuation[7:]\n",
    "filtered = list(punctuation) + ['https','http','//t.co'] + list(stopwords)\n",
    "# filter bigrams\n",
    "tokens_ = [x for x in tokens if x not in filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot distribution of bigram frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find 50 most common bigrams\n",
    "bigramfreq = nltk.FreqDist(nltk.bigrams(tokens_))\n",
    "bigramfreq.most_common(20)\n",
    "frequencies = [freq for bigram, freq in bigramfreq.items()]\n",
    "# plot distribution\n",
    "plt.hist(frequencies, bins=100)\n",
    "plt.title(\"Distribution of Bigram Frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Count important bigrams **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams_sorted = sorted(bigramfreq.items(), key=lambda x: -x[1])\n",
    "print bigrams_sorted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram is important if it's associated more than 50 times\n",
    "important_bigrams = [(bigram, val) for bigram, val in bigrams_sorted if val >= 50]\n",
    "len(important_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of our bigrams are important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frac = len(important_bigrams) / float(len(bigrams))\n",
    "print frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us hope that the presence of bigrams won't throw off our calculations too badly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Manual scoring differ from our unigram scores? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign scores to what we think is appropriate\n",
    "manual_scores = bigrams_sorted[:20]\n",
    "bigrams_tuple = [str(bigram) for bigram, frequency in manual_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually score the bigrams to see which are positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigramdf = pd.DataFrame.from_items([('bigrams', bigrams_tuple)])\n",
    "manual_ratings = [\"Pos\", \"Pos\", \"Pos\", \"Neg\", \"Neg\", \"Pos\", \"Neg\", \"Neg\", \"Pos\", \"Pos\", \"Pos\", \n",
    "                  \"Neg\", \"Neg\", \"Neg\", \"Neg\", \"Pos\", \"Pos\", \"Pos\", \"Pos\", \"Neg\"]\n",
    "bigramdf['manual_ratings']= manual_ratings\n",
    "bigramdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do some unigram scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scoring function for lists\n",
    "def score(words):\n",
    "    return sum([happiness.get(word.lower(), 0.0) for word in words]) / len(words)\n",
    "\n",
    "def scoreNoNeutrals(words):\n",
    "    notscored = [word for word in words if happiness.get(word.lower(), 0.0) == 0.0]\n",
    "    return sum([happiness.get(word.lower(), 0.0) for word in words]) / max((len(words) - len(notscored)),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams_text = [[word1, word2] for (word1, word2), frequency in manual_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our unigram scores including neutrals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pass bigrams to score function\n",
    "unigram_scores_neutrals = [sum([happiness.get(word.lower(), 0.0) for word in bigram]) / len(bigram) for bigram in bigrams_text]\n",
    "# # print whether they're positive or neutral\n",
    "unigram_bool_neutrals = [\"Pos\" if score > 0 else \"Neg\" for score in unigram_scores_neutrals]\n",
    "bigramdf['unigram_ratings_neutrals']= unigram_bool_neutrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our unigram scores without neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pass bigrams to no neutrals score function\n",
    "unigram_scores_no_neutrals = [scoreNoNeutrals(bigram) for bigram in bigrams_text]\n",
    "# print whether they're positive or neutral\n",
    "unigram_bool_no_neutrals = [\"Pos\" if score > 0 else \"Neg\" for score in unigram_scores_neutrals]\n",
    "bigramdf['unigram_ratings_no_neutrals']= unigram_bool_no_neutrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bigramdf now includes manual_ratings, unigram_ratings with neutrals and unigram_ratings without neutrals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigramdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of our unigram scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate percent difference neutrals\n",
    "bigramdf['neutral_manual_same'] = [ 1 if manual == neutral else 0 for manual, neutral in zip(\n",
    "                                    bigramdf['manual_ratings'],\n",
    "                                    bigramdf['unigram_ratings_neutrals'])]\n",
    "neutralcount = bigramdf['neutral_manual_same'].sum()\n",
    "# calculate percent difference no neutrals\n",
    "neutralcount/float(len(bigramdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that our unigram scoring seems pretty accurate since 90% of our manual scoring of bigrams actually match our unigram scores. Now we see if this holds for our no neutrals scoring as wel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noneutral_manual_same\n",
    "bigramdf['noneutral_manual_same'] = [ 1 if manual == neutral else 0 for manual, neutral in zip(\n",
    "                                    bigramdf['manual_ratings'],\n",
    "                                    bigramdf['unigram_ratings_no_neutrals'])]\n",
    "noneutralcount = bigramdf['noneutral_manual_same'].sum()\n",
    "noneutralcount/float(len(bigramdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see do the same thing with trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(tokens)\n",
    "# scoring the association\n",
    "# scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
    "# print scored[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finds most associated trigrams\n",
    "top_bigrams = finder.nbest(trigram_measures.raw_freq, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define bigrams\n",
    "trigrams = dftouse['text'].apply(lambda x: list(nltk.trigrams(nltk.word_tokenize(x.decode('utf-8','ignore')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finds the most common 20\n",
    "tokens_ = [x for x in tokens if x not in filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot distribution of trigram frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find 50 most common bigrams\n",
    "trigramfreq = nltk.FreqDist(nltk.trigrams(tokens_))\n",
    "trigramfreq.most_common(20)\n",
    "frequencies = [freq for trigram, freq in trigramfreq.items()]\n",
    "# plot distribution\n",
    "plt.hist(frequencies, bins=100)\n",
    "plt.title(\"Distribution of Trigram Frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Count important trigrams **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams_sorted = sorted(trigramfreq.items(), key=lambda x: -x[1])\n",
    "print trigrams_sorted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram is important if it's associated more than 50 times\n",
    "important_trigrams = [(trigram, val) for trigram, val in trigrams_sorted if val >= 50]\n",
    "len(important_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As before, the percentage of our trigrams that are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frac = len(important_trigrams) / float(len(trigrams))\n",
    "print frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare manual and unigram scores of trigrams.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign scores to what we think is appropriate\n",
    "manual_scores = trigrams_sorted[:20]\n",
    "trigrams_tuple = [str(trigram) for trigram, frequency in manual_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigramdf = pd.DataFrame.from_items([('trigrams', trigrams_tuple)])\n",
    "manual_ratings = [\"Pos\", \"Pos\", \"Pos\", \"Neg\", \"Neg\", \"Pos\", \"Neg\", \"Neg\", \"Pos\", \"Pos\", \"Pos\", \n",
    "                  \"Neg\", \"Neg\", \"Neg\", \"Neg\", \"Pos\", \"Pos\", \"Pos\", \"Pos\", \"Neg\"]\n",
    "trigramdf['manual_ratings']= manual_ratings\n",
    "trigramdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scoring function for lists\n",
    "def score(words):\n",
    "    return sum([happiness.get(word.lower(), 0.0) for word in words]) / len(words)\n",
    "\n",
    "def scoreNoNeutrals(words):\n",
    "    notscored = [word for word in words if happiness.get(word.lower(), 0.0) == 0.0]\n",
    "    return sum([happiness.get(word.lower(), 0.0) for word in words]) / max((len(words) - len(notscored)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams_text = [[word1, word2, word3] for (word1, word2, word3), frequency in manual_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring with neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pass bigrams to score function\n",
    "unigram_scores_neutrals = [sum([happiness.get(word.lower(), 0.0) for word in trigram]) / len(bigram) for trigram in trigrams_text]\n",
    "# # print whether they're positive or neutral\n",
    "unigram_bool_neutrals = [\"Pos\" if score > 0 else \"Neg\" for score in unigram_scores_neutrals]\n",
    "trigramdf['unigram_ratings_neutrals']= unigram_bool_neutrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring without neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pass bigrams to no neutrals score function\n",
    "unigram_scores_no_neutrals = [scoreNoNeutrals(bigram) for bigram in bigrams_text]\n",
    "# print whether they're positive or neutral\n",
    "unigram_bool_no_neutrals = [\"Pos\" if score > 0 else \"Neg\" for score in unigram_scores_neutrals]\n",
    "bigramdf['unigram_ratings_no_neutrals']= unigram_bool_no_neutrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of scoring with and without neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate percent difference neutrals\n",
    "trigramdf['neutral_manual_same'] = [ 1 if manual == neutral else 0 for manual, neutral in zip(\n",
    "                                    trigramdf['manual_ratings'],\n",
    "                                    trigramdf['unigram_ratings_neutrals'])]\n",
    "neutralcount = trigramdf['neutral_manual_same'].sum()\n",
    "# noneutral_manual_same\n",
    "bigramdf['noneutral_manual_same'] = [ 1 if manual == neutral else 0 for manual, neutral in zip(\n",
    "                                    bigramdf['manual_ratings'],\n",
    "                                    bigramdf['unigram_ratings_no_neutrals'])]\n",
    "noneutralcount = bigramdf['noneutral_manual_same'].sum()\n",
    "# calculate percent difference neutrals, and no neutrals\n",
    "neutralcount/float(len(trigramdf)), noneutralcount/float(len(bigramdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length of post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controversy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# another example with Cursor get all tweets with a certain hashtag and a certain time frame within past week \n",
    "# csvFile = open('tweets.csv', 'a')\n",
    "# Use csv Writer\n",
    "# csvWriter = csv.writer(csvFile)\n",
    "\n",
    "# for tweet in tweepy.Cursor(api.search,q=\"#PrayForJapan\",count=1,\\\n",
    "#                            lang=\"en\",\\\n",
    "#                            since_id=2015-11-13).items():\n",
    "#     print tweet.created_at, tweet.text\n",
    "#     csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
